from layers import *
import numpy as np


class FullyConnectedNet:
    """
    A fully-connected neural network with an arbitrary number of hidden layers,
    ReLU nonlinearities, and a softmax as last layer. This will also implement
    dropout and batch normalization as options.
    """
    
    def __init__(self, layer_dims, input_dim=3*32*32, num_classes=10,
                 dropout=1, use_batchnorm=False):
        """
        Initialize a new FullyConnectedNet.
        
        Inputs:
        - layer_dims: A list of integers giving the size of each hidden layer.
        - input_dim: An integer giving the size of the input.
        - num_classes: An integer giving the number of classes to classify.
        - dropout: Scalar between 0 and 1 giving dropout strength. If dropout=1 then the network should not use dropout at all.
        - normalization: a boolean defines whether or not use batch norm
        """
        np.random.seed(42)
        self.params = {}
        self.input_dim = input_dim
        self.num_classes = num_classes
        self.use_batchnorm = use_batchnorm
        self.use_dropout = dropout != 1
        self.layer_dims = layer_dims
        self.num_layers = 1 + len(layer_dims)
        self.initialize_parameters()


    def L_model_forward(self, X):
        """
        forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX computation

        :param X: the data, numpy array of shape (input size, number of examples)
        :param parameters: the initialized W and b parameters of each layer
        :param use_batchnorm: a boolean flag used to determine whether to apply batchnorm after the activation
        :return: (the last post-activation value , a list of all the cache objects)
        """

        layer_input = X
        caches = []
        for layer_idx in range(self.num_layers - 1):
            W, b = self.parameters['W' + str(layer_idx+1)], self.parameters['b' + str(layer_idx+1)]
            layer_input, layer_cache = linear_activation_forward(layer_input, W, b, 'relu')
            caches.append(layer_cache)
            if self.use_batchnorm:
                layer_input = apply_batchnorm(layer_input)

        # last layer
        W, b = self.parameters['W' + str(self.num_layers)], self.parameters['b' + str(self.num_layers)]
        last_post_activation, layer_cache = linear_activation_forward(layer_input, W, b, 'softmax')
        caches.append(layer_cache)

        return last_post_activation, caches

    def L_model_backward(self, AL, Y, caches):
        """
        Backward propagation process for the entire network.

        :param AL: the probabilities vector, the output of the forward propagation (L_model_forward)
        :param Y: the true labels vector (the "ground truth" - true classifications)
        :param caches: list of caches containing for each layer: a) the linear cache; b) the activation cache
        :return: a dictionary with the gradients
        """

        grads = {}
        last_layer_idx = self.num_layers - 1

        # dL / dA = -(Y/A) + ((1-Y)/1-A)
        #TODO: fix parameters for softmax_backward (what should be dA)
        last_layer_dA = softmax_backward()
        grads['dA' + str(last_layer_idx)] = last_layer_dA

        dA, dW, db = linear_activation_backward(last_layer_dA, caches[last_layer_idx], 'sigmoid')
        grads['dW' + str(last_layer_idx)] = dW
        grads['db' + str(last_layer_idx)] = db

        for layer_idx in reversed(range(self.num_layers - 1)):
            grads['dA' + str(layer_idx)] = dA

            dA, dW, db = linear_activation_backward(dA , caches[layer_idx], "relu")
            grads['dW' + str(layer_idx)] = dW
            grads['db' + str(layer_idx)] = db

        return grads

    def update_parameters(self, parameters, grads, learning_rate):
        """
        Updates parameters using gradient descen

        :param parameters: a python dictionary containing the DNN architecture’s parameters
        :param grads: a python dictionary containing the gradients (generated by L_model_backward)
        :param learning_rate: the learning rate used to update the parameters (the “alpha”)
        :return: – the updated values of the parameters object provided as input
        """

        for layer_idx in range(self.num_layers):
            old_W, dW = parameters['W' + str(layer_idx)], grads['db' + str(layer_idx)]
            old_b, db = parameters['b' + str(layer_idx)], grads['dW' + str(layer_idx)]

            parameters['W' + str(layer_idx)] = old_W - learning_rate * dW
            parameters['b' + str(layer_idx)] = learning_rate * db

        return parameters

    def L_layer_model(self, X, Y, layers_dims, learning_rate, num_iterations, batch_size):
        """

        :param X: the input data, a numpy array of shape (height*width , number_of_examples)
        :param Y: the “real” labels of the data, a vector of shape (num_of_classes, number of examples)
        :param layers_dims: a list containing the dimensions of each layer, including the input
        :param learning_rate: the learning rate
        :param num_iterations: number of iterations
        :param batch_size: the number of examples in a single training batch.
        :return: (parameters, costs) - the parameters learnt by the system during the training (the same parameters
                                        that were updated in the update_parameters function) and the values of the cost
                                        function (calculated by the compute_cost function). One value is to be saved
                                        after each 100 training iterations (e.g. 3000 iterations -> 30 values)..
        """
        # initialization
        self.initialize_parameters()
        costs = []

        for i in range(num_iterations):
            # choose the batch
            batch_idx = np.random.choice(np.arange(X.shape[1]), size=batch_size, replace=False)
            X_batch, Y_batch = X[:, batch_idx], Y[batch_idx]

            # forward pass
            AL, caches = self.L_model_forward(X_batch, parameters, False)

            # compute the cost and document it
            cost = compute_cost(AL, Y_batch)
            if i % 100 == 0:
                costs.append(cost)

            # backward pass
            grads = self.L_model_backward(AL, Y, caches)

            # update parameters
            parameters = self.update_parameters(parameters, grads, learning_rate)

        return parameters, costs


    def initialize_parameters(self):
        """
        input:
            an array of the dimensions of each layer in the network (layer 0 is the size of the flattened input, layer L is the output softmax)
        output:
            a dictionary containing the initialized W and b parameters of each layer (W1...WL, b1...bL).
        """
        layer_input_dim = self.input_dim

        # input-> hidden_layer_1 -> hidden_layer_2 -> ... -> hidden_layer_last
        for idx, dim in enumerate(self.layer_dims): # enumrate all hidden layers
            layer_num = str(idx+1)
            self.params['W' + layer_num] = np.random.randn(layer_input_dim, dim)
            self.params['b' + layer_num] = np.zeros(dim)
            layer_input_dim = dim

        # hidden_layer_last -> output
        num_layers = len(self.layer_dims)
        self.params['W' + str(num_layers)] = np.random.randn(layer_input_dim, self.num_classes)
        self.params['b' + str(num_layers)] = np.zeros(self.num_classes)


def compute_cost(AL, Y):
    """
    Implement the cost function defined by equation. The requested cost function is categorical cross-entropy loss.

    :param AL: – probability vector corresponding to your label predictions, shape (num_of_classes, number of examples)
    :param Y: the labels vector (i.e. the ground truth)
    :return: the cross-entropy cost
    """

    #TODO: check what happen when AL got invalid value for log
    return -np.divide( np.sum((Y * np.log(AL)).sum(axis=0)), Y.shape[0])
    #return -np.sum((Y * np.log(AL)) + ((1-Y) * np.log(1-AL))) / Y.shape[0]


def predict(X, Y, parameters):
    """
    Description:
        The function receives an input data and the true labels and calculates the accuracy of
        the trained neural network on the data.
    Input:
        X – the input data, a numpy array of shape (height*width, number_of_examples)
        Y – the “real” labels of the data, a vector of shape (num_of_classes, number of examples)
        parameters – a python dictionary containing the DNN architecture’s parameters
    Output:
        accuracy – the accuracy measure of the neural net on the provided data (i.e. the
        percentage of the samples for which the correct label receives over 50% of the
        confidence score). Use the softmax function to normalize the output values.
    """
    # scores: Array of shape (num_classes, number_of_examples) giving classification scores,
    # where scores[c, i] is the classification score for X[i] and class c.
    scores = L_model_forward(X, parameters, use_batchnorm=False) # TODO: ask Gilad where use_batchnorm should come from.
    probs = softmax(scores)
    labels_over_50 = np.where()

    # compare y and y_pred


def apply_batchnorm(activation):
    epsilon = 0.000001
    miu = np.sum(activation) / activation.shape[0]
    sigma = (np.sum(activation - miu) ** 2) / activation.shape[0]
    return (activation - miu) / (sigma + epsilon) ** 0.5
